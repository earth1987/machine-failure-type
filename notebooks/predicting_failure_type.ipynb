{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c525022-e2a7-4328-b74c-ea12e68bbbe4",
   "metadata": {},
   "source": [
    "# Predicting machine failure\n",
    "\n",
    "Predicting machine failure through machine learning is paramount for efficient industrial operations. By analysing data patterns, ML models can forecast potential breakdowns, allowing for proactive maintenance and minimising downtime. This predictive approach not only enhances equipment reliability but also optimises resource allocation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88362d1-13f4-4690-a66d-dee97f524ca1",
   "metadata": {},
   "source": [
    "## Scope\n",
    "\n",
    "**_Context_**\n",
    "\n",
    "* [The AI4I 2020 Predictive Maintenance Dataset](https://archive.ics.uci.edu/dataset/601/ai4i+2020+predictive+maintenance+dataset) is a synthetic dataset that reflects real predictive maintenance data encountered in industry.\n",
    "\n",
    "* It will be assumed that faulty machinery would lead to decreased productivity and efficiency, quality issues, defective products, and potentially safety concerns for workers.\n",
    "\n",
    "**_Model_**\n",
    "\n",
    "* The machine failure consists of five independent failure modes. Consequently, the problem will be approached as multiclass classification.\n",
    "\n",
    "* No baseline model is available for benchmarking. The model will therefore be compared to a no-skill classifier.\n",
    "\n",
    "**_Technical requirements_**\n",
    "\n",
    "* In reality, the model would likely be deployed to an online endpoint so predictions could be generated in real time (via a REST API).\n",
    "\n",
    "* At this stage, the model will not be deployed. However, the low latency requirements of online deployment does place constraints on infrastructure and model complexity. For this reason, neural networks will not be tested as they can be slow at inference time due to the large number of operations.\n",
    "\n",
    "**_Data requirements_**\n",
    "\n",
    "* No further data collection is required.\n",
    "\n",
    "* No personal data is involved. Consequently there are no apparent legal/ethical constraints (e.g. GDPR)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a898fe-ded7-4c4b-ae56-299bede96f51",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "**_What is multi-class classification?_**\n",
    "\n",
    "* A classification task with more than two classes. Each sample can only be labeled as one class.\n",
    "\n",
    "    Note: Multiclass classification is different to multi-label classification, where multiple labels are to be predicted for each instance.\n",
    "\n",
    "**_Strategies_**\n",
    "\n",
    "* Multi-class classification techniques can be categorised into two categories:\n",
    "\n",
    "    1. **Transformation to binary**\n",
    "\n",
    "       Strategy: Decompose the problem into multiple binary classification subproblems.\n",
    "      \n",
    "    2. **Extension from binary**\n",
    " \n",
    "       Strategy: Extend an existing binary classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d67bf8-ba5f-4b76-84cd-a2b82c3af5e8",
   "metadata": {},
   "source": [
    "### Transformation to binary\n",
    "\n",
    "**a) OVO: One vs. One**\n",
    "\n",
    "Set-up:\n",
    "\n",
    "* Targets form a discrete set\n",
    "\n",
    "  $y \\in \\{1, 2,...,K\\}$\n",
    "\n",
    "    *Where $K$ is the number of classes.*\n",
    "\n",
    "Training:\n",
    "\n",
    "* A binary classifier is trained for every pair of classes:\n",
    "\n",
    "    $\\text{No. of classifiers}=\\frac{k \\times (k-1)}{2!}$\n",
    "\n",
    "* A subset of the training data, containing only the two classes, is used during training.\n",
    "\n",
    "Prediction:\n",
    "\n",
    "* All classifiers are used and a voting scheme applied.\n",
    "\n",
    "* The class with the most votes is chosen as the final prediction:\n",
    "\n",
    "    $\\hat{y}_{final} = \\underset{k}{\\operatorname{argmax}} \\sum_i 1\\bigl(h_i(\\boldsymbol{x})=k\\bigr)$\n",
    " \n",
    "    *Where $h_i(\\boldsymbol{x})$ is the output of classifier $i$ and $1\\bigl(h_i(\\boldsymbol{x})=k\\bigr)$ returns $1$ when it returns classification $k$ and $0$ otherwise.*\n",
    "\n",
    "Example:\n",
    "\n",
    "* Three classes $\\{A, B, C\\}$\n",
    "\n",
    "* The following binary classifiers are trained:\n",
    "\n",
    "  $\\text{Classifier 1} = \\text{A vs. B}$\n",
    "\n",
    "  $\\text{Classifier 2} = \\text{A vs. C}$\n",
    "  \n",
    "  $\\text{Classifier 3} = \\text{B vs. C}$\n",
    "\n",
    "Drawbacks:\n",
    "\n",
    "* Some classes may receive the same number of votes.\n",
    "\n",
    "  Resolving ties requires choosing the class with the highest total confidence or probability across all classifiers.\n",
    "\n",
    "* Computationally demanding. It requires $O(k^2)$ classifiers.\n",
    "\n",
    "  The number of classifiers required is high."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa5f834-ee24-4c81-9a89-b96fb7e8f149",
   "metadata": {},
   "source": [
    "**b) OVR: One vs. Rest**\n",
    "\n",
    "* Also known as OVA (One vs. All)\n",
    "\n",
    "Set-up:\n",
    "\n",
    "* Targets form a discrete set\n",
    "\n",
    "  $y \\in \\{1, 2,...,K\\}$\n",
    "\n",
    "    *Where $K$ is the number of classes.*\n",
    "\n",
    "Training:\n",
    "\n",
    "* All training data is used.\n",
    "\n",
    "* A binary classifier is trained for each class against the rest of the classes.\n",
    "\n",
    "    $\\text{No. of classifiers}=K$\n",
    "\n",
    "    Note: It requires $O(K)$ classifiers. This means OVR is more efficient than OVO.\n",
    "\n",
    "Prediction:\n",
    "\n",
    "* All classifiers are utilised.\n",
    "\n",
    "* The final prediction is the class for which the corresponding classifier reports the highest confidence score:\n",
    "\n",
    "    $\\hat{y}_{final} = \\underset{k \\in \\{1...K\\}}{\\operatorname{argmax}} h_k(\\boldsymbol{x})$\n",
    " \n",
    "    *Where $h_k(\\boldsymbol{x})$ is the output of classifier $k$.*\n",
    "\n",
    "Example:\n",
    "\n",
    "* Three classes $\\{A, B, C\\}$\n",
    "\n",
    "* The following binary classifiers are trained:\n",
    "\n",
    "  $\\text{Classifier 1} = \\text{A vs. (B,C)}$\n",
    "\n",
    "  $\\text{Classifier 2} = \\text{B vs. (A,C)}$\n",
    "  \n",
    "  $\\text{Classifier 3} = \\text{C vs. (A, B)}$\n",
    "\n",
    "Drawbacks:\n",
    "\n",
    "* Aggregation of different classes leads to imbalanced datasets and sometimes suboptimal performance on the minority target class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4050ba6b-ca67-4f41-b09c-904d69222358",
   "metadata": {},
   "source": [
    "### Extension from binary\n",
    "\n",
    "These strategies extend existing binary classifiers to solve multi-class classification problems.\n",
    "\n",
    "**a) Naive Bayes**\n",
    "\n",
    "* The algorithm can naturally handle binary or multiclass classification problems\n",
    "\n",
    "* Simply choose the classification $k$ that maximises $p(C_k | \\boldsymbol{x})$:\n",
    "\n",
    "    $\\hat{y} = \\underset{k \\in 1...K}{\\operatorname{argmax}} p(C_k|\\boldsymbol{x})$\n",
    "\n",
    "    $\\hat{y} = \\underset{k \\in 1...K}{\\operatorname{argmax}} p(C_k)\\prod_{i=1}^n p(x_i | C_k)$\n",
    "\n",
    "    *Where $\\boldsymbol{x}$ is the vector of $n$ features for a single test instance being classified.*\n",
    " \n",
    "**b) Softmax regression (or multinomial logistic regression)**\n",
    "\n",
    "* Softmax regression  is a generalisation of logistic regression to a multiclass problem\n",
    "\n",
    "Logistic regression:\n",
    "\n",
    "* One weight vector $\\theta$ is learnt.\n",
    "\n",
    "    *Note: Weights are computed using the log-likelihood function*\n",
    "\n",
    "* The log-odds $z$ (AKA logit) for a single data point $(\\boldsymbol{x},y)$ is given by a linear combination of features $x_i$ and weights $\\theta_i$:\n",
    "\n",
    "    $z=\\sum_{i=0}^{m}\\theta_ix_i = \\theta^T\\boldsymbol{x}$\n",
    "\n",
    "    $log\\bigl(odds\\bigr) = log\\bigl(\\frac{P(Y = 1|\\boldsymbol{X} = \\boldsymbol{x})}{1 - P(Y = 1|\\boldsymbol{X} = \\boldsymbol{x})}\\bigr) = \\theta^T\\boldsymbol{x}$\n",
    "\n",
    "    *Where, $\\theta$ is a vector of parameters of length $m$ and $\\boldsymbol{x}$ is a vector of values for each predictor variable (note: $x_0$ is always set to $1$)*.\n",
    "\n",
    "* $P(Y|X)$ can then be approximated by applying a sigmoid function to the log-odds:\n",
    "\n",
    "    $\\sigma(z)=\\frac{1}{1+e^{-z}}$, where $\\forall z \\in R$, $\\sigma(z) \\in [0,1]$\n",
    "\n",
    "    $P(Y = 1|\\boldsymbol{X} = \\boldsymbol{x}) = \\sigma(z)$\n",
    "\n",
    "    $P(Y = 1|\\boldsymbol{X} = \\boldsymbol{x}) = \\sigma(\\theta^T\\boldsymbol{x})$\n",
    "\n",
    "    $P(Y = 1|\\boldsymbol{X} = \\boldsymbol{x}) = \\frac{1}{1+exp(-\\theta^T\\boldsymbol{x})}$\n",
    "\n",
    "    *Where, $\\sigma(\\cdot)$ is the sigmoid function.*\n",
    "\n",
    "* The decision rule is based on whether the predicted probability is greater than a specified threshold.\n",
    "\n",
    "    $ \\hat{y} = \\begin{cases} \n",
    "    1 & \\text{if } P(Y = 1|\\boldsymbol{X} = \\boldsymbol{x}) \\geq 0.5 \\\\\n",
    "    0 & \\text{if } P(Y = 1|\\boldsymbol{X} = \\boldsymbol{x}) < 0.5\n",
    "    \\end{cases}$\n",
    "\n",
    "Softmax regression:\n",
    "\n",
    "* One set of weights is learnt for each class $\\theta^{(k)}$\n",
    "\n",
    "    *Note: Weights are computed at the same time using a vector objective function.*\n",
    "\n",
    "* For a single data point $(\\boldsymbol{x},y)$, the logit for each class $z^{(k)}$ is given by a linear combination of features $x_i$ and the relevant weights $\\theta^{(k)}$:\n",
    "\n",
    "    $z^{(k)}=\\sum_{i=0}^{m}\\theta_i^{(k)}x_i = \\theta^{(k)T}\\boldsymbol{x}$\n",
    "\n",
    "    *Where, $\\theta$ is a vector of parameters of length $m$ and $\\boldsymbol{x}$ is a vector of values for each predictor variable (note: $x_0$ is always set to $1$)*.\n",
    "\n",
    "* The softmax function is then applied to these raw scores to obtain normalized class probabilities:\n",
    "\n",
    "    $P(y=k | \\boldsymbol{x}; \\theta) = \\frac{exp\\bigl(z^{(k)}\\bigr)}{\\sum_{j=1}^K exp\\bigl(z^{(j)}\\bigr)}$\n",
    "\n",
    "    $P(y=k | \\boldsymbol{x}; \\theta) = \\frac{exp\\bigl(\\theta^{(k)T}\\boldsymbol{x}\\bigr)}{\\sum_{j=1}^K exp\\bigl(\\theta^{(j)T}\\boldsymbol{x}\\bigr)}$\n",
    "    \n",
    "* The decision rule is based on selecting the class with the highest predicted probability:\n",
    "\n",
    "    $\\hat{y}_{final} = \\underset{k \\in \\{1...K\\}}{\\operatorname{argmax}} P(y=k | \\boldsymbol{x}; \\theta)$\n",
    "\n",
    "[Source](http://deeplearning.stanford.edu/tutorial/supervised/SoftmaxRegression/)\n",
    "\n",
    "**c) Random forest**\n",
    "\n",
    "* The algorithm can naturally handle binary or multiclass classification problems.\n",
    "\n",
    "* Decicision rules are learnt by repeatedly partitioning training data using predictor values that maximise within partition homogeneity (i.e. class purity) for classification tasks\n",
    "\n",
    "* The class with the most votes is chosen:\n",
    "\n",
    "    $\\hat{y}_{final} = \\underset{k}{\\operatorname{argmax}} \\sum_i 1\\bigl(h_i(\\boldsymbol{x})=k\\bigr)$\n",
    " \n",
    "    *Where $h_i(\\boldsymbol{x})$ is the output of tree $i$ and $1\\bigl(h_i(\\boldsymbol{x})=k\\bigr)$ returns $1$ when it returns classification $k$ and $0$ otherwise.*\n",
    "\n",
    "**d) XGBoost**\n",
    "\n",
    "* The idea of boosting methods is to combine several weak learners to form a stronger one.\n",
    "\n",
    "* In gradient boosting, weak learners are trained sequentially on residuals\n",
    "\n",
    "Binary setting:\n",
    "\n",
    "* The final model is a stagewise additive model of $T$ individual trees\n",
    "\n",
    "* The log-odds $z$ (AKA logit) for a single data point $(\\boldsymbol{x},y)$ is given by the sum of the sequence of weak learners:\n",
    "\n",
    "    $z = H_T(\\boldsymbol{x}) = \\sum_{t=1}^T h_t(\\boldsymbol{x})$\n",
    "\n",
    "  *Where $H_T$ is the ensemble and $T$ is number of consitutent learners*\n",
    "\n",
    "* The logit is transformed into a probability using a sigmoid function.\n",
    "\n",
    "    $P(Y = 1|\\boldsymbol{X} = \\boldsymbol{x}) = \\frac{1}{1+exp(-z)}$\n",
    "\n",
    "    *Where, $\\sigma(\\cdot)$ is the sigmoid function.*\n",
    "\n",
    "* The decision rule is based on whether the predicted probability is greater than a specified threshold.\n",
    "\n",
    "    $ \\hat{y} = \\begin{cases} \n",
    "    1 & \\text{if } P(Y = 1|\\boldsymbol{X} = \\boldsymbol{x}) \\geq 0.5 \\\\\n",
    "    0 & \\text{if } P(Y = 1|\\boldsymbol{X} = \\boldsymbol{x}) < 0.5\n",
    "    \\end{cases}$\n",
    "\n",
    "Multiclass setting:\n",
    "\n",
    "* Distinct weak learners are trained for each class\n",
    "\n",
    "    *Note: In reality, these are computed at the same time using a vector objective function.*\n",
    "\n",
    "* The predictions from all class-specific trees are aggregated to form a final score for each class.\n",
    "\n",
    "    $z^{(k)} = H_T^{(k)}(\\boldsymbol{x}) = \\sum_{t=1}^T h_t^{(k)}(\\boldsymbol{x})$\n",
    "\n",
    "  *Where $H_T^{(k)}$ is the ensemble for class $k$ and $T$ is number of consitutent learners*\n",
    "\n",
    "* The softmax function is then applied to these raw scores to obtain normalized class probabilities:\n",
    "\n",
    "    $P(y=k | \\boldsymbol{x}; \\theta) = \\frac{exp\\bigl(z^{(k)}\\bigr)}{\\sum_{j=1}^K exp\\bigl(z^{(j)}\\bigr)}$\n",
    "  \n",
    "* The decision rule is based on selecting the class with the highest predicted probability:\n",
    "\n",
    "    $\\hat{y}_{final} = \\underset{k \\in \\{1...K\\}}{\\operatorname{argmax}} P(y=k | \\boldsymbol{x}; \\theta)$\n",
    "\n",
    "[Source](https://stats.stackexchange.com/questions/204154/classification-with-gradient-boosting-how-to-keep-the-prediction-in-0-1)\n",
    "\n",
    "[Source](https://bradleyboehmke.github.io/HOML/gbm.html#feature-interpretation-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffec3b05-49c1-4bf2-931b-d58f6db6ea99",
   "metadata": {},
   "source": [
    "## Strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0758941-9fe7-4854-87ce-bf1c8f298700",
   "metadata": {},
   "source": [
    "### Model types\n",
    "\n",
    "* An \"extension from binary\" strategy will be used.\n",
    "\n",
    "* This will prevent loss of information about the multiclass relationships and reduce model complexity (i.e. one-vs-one would result in a large number of binary classifiers.)\n",
    "\n",
    "* A Random Forest will be tested as this type of model performed the best for the binary classification problem.\n",
    "  \n",
    "    - Random Forest (discriminative probabilistic model & non-linear classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e34fcdd-35bb-4861-9ab5-11f2f0f04409",
   "metadata": {},
   "source": [
    "### Performance metrics\n",
    "\n",
    "**Background**\n",
    "\n",
    "* In the context of a multiclass problem, micro and macro averaging are commonly used to summarise performance across multiple classes.\n",
    "\n",
    "* Micro-averaging considers all instances equally and is therefore more influenced by the performance on larger classes.\n",
    "\n",
    "    $Micro-Recall = \\frac{\\sum_{i=1}^KTP_i}{\\sum_{i=1}^K\\bigl(TP_i + FN_i\\bigr)}$\n",
    "\n",
    "    $Micro-Precision = \\frac{\\sum_{i=1}^KTP_i}{\\sum_{i=1}^K\\bigl(TP_i + FP_i\\bigr)}$\n",
    "\n",
    "    $Micro-F1 = \\frac{2 \\times micro-recall \\times micro-precision}{micro-recall + micro-precision}$\n",
    "\n",
    "    *Where $K$ is the number of classes.*\n",
    "\n",
    "* Whilst macro-averaging treats all classes equally and therefore gives a more balanced view of performance across all classes.\n",
    "\n",
    "    $Macro-Recall = \\frac{1}{K}\\sum_{i=1}^K\\frac{TP_i}{TP_i + FN_i}$\n",
    "\n",
    "    $Macro-Precision = \\frac{1}{K}\\sum_{i=1}^K\\frac{TP_i}{TP_i + FP_i}$\n",
    "\n",
    "    $Macro-F1 = \\frac{2}{K}\\sum_{i=1}^K\\frac{micro-recall_i \\times micro-precision_i}{micro-recall_i + micro-precision_i}$\n",
    "\n",
    "    *Where $K$ is the number of classes.*\n",
    "\n",
    "**Strategy**\n",
    "\n",
    "* The performance metrics should align closely with the specific business problem at hand.\n",
    "\n",
    "* While a single metric simplifies ranking model performance, the dataset's imbalance makes overall accuracy unsuitable.\n",
    "\n",
    "* It is assumed the cost of a false negative prediction (incorrectly identifying something as okay when it has failed) outweighs the cost of a false positive prediction.\n",
    "\n",
    "* The primary metric is therefore macro-recall across the failure classes, reflecting the model's ability to identify failed machinery and aiming to minimise false negatives.\n",
    "\n",
    "* A secondary metric is macro-precision across the failure classes, thereby minimising false positives and ensuring high accuracy in failed predictions. The optimisation target is a nominal constraint of 50% precision.\n",
    "\n",
    "* Striking a balance involves achieving a reasonable level of precision to prevent unnecessary disruptions while maintaining a sufficiently high recall for effective fault detection. With this in mind, Macro-F1 score will be used during optimisation.\n",
    "\n",
    "    Note: AUC on the Precision-Recall for each class would allow for the comparison of different models in terms of their ability to balance precision and recall. However, this reduces the problem to an OVA strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eaf31f9-60fb-4205-a35f-798bec1a456d",
   "metadata": {},
   "source": [
    "## Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dc5616ee-1feb-43ee-988e-55bfb6b978f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import seaborn as sns\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, \"C:\\\\Users\\\\billy\\\\OneDrive\\\\Documents\\\\Python Scripts\\\\1. Portfolio\\\\machine-failure\\\\machine-failure\")\n",
    "import custom_funcs as cf\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import make_scorer, f1_score, precision_score, recall_score\n",
    "import collections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059c9f9e-c9cb-4d50-8877-a2a5adaf0df1",
   "metadata": {},
   "source": [
    "## Import raw data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61bc7b7-017e-4b79-b54d-c338cb578d5a",
   "metadata": {},
   "source": [
    "* **UID:** Unique identifier ranging from 1 to 10000\n",
    "\n",
    "* **productID:** Consisting of a letter L, M, or H for low (50% of all products), medium (30%), and high (20%) as product quality variants and a variant-specific serial number\n",
    "\n",
    "* **air temperature [K]:** Generated using a random walk process later normalized to a standard deviation of 2 K around 300 K\n",
    "\n",
    "* **process temperature [K]:** Generated using a random walk process normalized to a standard deviation of 1 K, added to the air temperature plus 10 K.\n",
    "\n",
    "* **rotational speed [rpm]:** Calculated from powepower of 2860 W, overlaid with a normally distributed noise\n",
    "\n",
    "* **torque [Nm]:** Torque values are normally distributed around 40 Nm with an Ïƒ = 10 Nm and no negative values.\n",
    "\n",
    "* **tool wear [min]:** The quality variants H/M/L add 5/3/2 minutes of tool wear to the used tool in the process. and a 'machine failure' label that indicates, whether the machine has failed in this particular data point for any of the following failure modes are true."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3393cb-9795-455c-bb10-5087f4935537",
   "metadata": {},
   "source": [
    "* **Target:** Failure or Not\n",
    "\n",
    "* **Failure Type:** Type of Failure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a7e187d-945c-48b4-b8a0-0a9cced38835",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UDI</th>\n",
       "      <th>Product ID</th>\n",
       "      <th>Type</th>\n",
       "      <th>Air temperature [K]</th>\n",
       "      <th>Process temperature [K]</th>\n",
       "      <th>Rotational speed [rpm]</th>\n",
       "      <th>Torque [Nm]</th>\n",
       "      <th>Tool wear [min]</th>\n",
       "      <th>Target</th>\n",
       "      <th>Failure Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>M14860</td>\n",
       "      <td>M</td>\n",
       "      <td>298.1</td>\n",
       "      <td>308.6</td>\n",
       "      <td>1551</td>\n",
       "      <td>42.8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>No Failure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>L47181</td>\n",
       "      <td>L</td>\n",
       "      <td>298.2</td>\n",
       "      <td>308.7</td>\n",
       "      <td>1408</td>\n",
       "      <td>46.3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>No Failure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>L47182</td>\n",
       "      <td>L</td>\n",
       "      <td>298.1</td>\n",
       "      <td>308.5</td>\n",
       "      <td>1498</td>\n",
       "      <td>49.4</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>No Failure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>L47183</td>\n",
       "      <td>L</td>\n",
       "      <td>298.2</td>\n",
       "      <td>308.6</td>\n",
       "      <td>1433</td>\n",
       "      <td>39.5</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>No Failure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>L47184</td>\n",
       "      <td>L</td>\n",
       "      <td>298.2</td>\n",
       "      <td>308.7</td>\n",
       "      <td>1408</td>\n",
       "      <td>40.0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>No Failure</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UDI Product ID Type  Air temperature [K]  Process temperature [K]  \\\n",
       "0    1     M14860    M                298.1                    308.6   \n",
       "1    2     L47181    L                298.2                    308.7   \n",
       "2    3     L47182    L                298.1                    308.5   \n",
       "3    4     L47183    L                298.2                    308.6   \n",
       "4    5     L47184    L                298.2                    308.7   \n",
       "\n",
       "   Rotational speed [rpm]  Torque [Nm]  Tool wear [min]  Target Failure Type  \n",
       "0                    1551         42.8                0       0   No Failure  \n",
       "1                    1408         46.3                3       0   No Failure  \n",
       "2                    1498         49.4                5       0   No Failure  \n",
       "3                    1433         39.5                7       0   No Failure  \n",
       "4                    1408         40.0                9       0   No Failure  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_df = pd.read_csv(cf.file_directory(\"raw\") + \"predictive_maintenance.csv\")\n",
    "raw_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976799d6-55ee-4b85-a735-09ad256d24a4",
   "metadata": {},
   "source": [
    "## Import cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93db2577-4ab4-403a-8b83-a87eccfcd690",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Process temperature [K]</th>\n",
       "      <th>Rotational speed [rpm]</th>\n",
       "      <th>Torque [Nm]</th>\n",
       "      <th>Tool wear [min]</th>\n",
       "      <th>Type_H</th>\n",
       "      <th>Type_L</th>\n",
       "      <th>Type_M</th>\n",
       "      <th>Target</th>\n",
       "      <th>Failure Type_Heat</th>\n",
       "      <th>Failure Type_None</th>\n",
       "      <th>Failure Type_Overstrain</th>\n",
       "      <th>Failure Type_Power</th>\n",
       "      <th>Failure Type_Random</th>\n",
       "      <th>Failure Type_Tool</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>310.7</td>\n",
       "      <td>1454</td>\n",
       "      <td>39.4</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>309.7</td>\n",
       "      <td>1868</td>\n",
       "      <td>23.8</td>\n",
       "      <td>118</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>308.5</td>\n",
       "      <td>1616</td>\n",
       "      <td>30.2</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>312.6</td>\n",
       "      <td>1768</td>\n",
       "      <td>23.9</td>\n",
       "      <td>149</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>313.4</td>\n",
       "      <td>1624</td>\n",
       "      <td>32.1</td>\n",
       "      <td>53</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Process temperature [K]  Rotational speed [rpm]  Torque [Nm]  \\\n",
       "0                    310.7                    1454         39.4   \n",
       "1                    309.7                    1868         23.8   \n",
       "2                    308.5                    1616         30.2   \n",
       "3                    312.6                    1768         23.9   \n",
       "4                    313.4                    1624         32.1   \n",
       "\n",
       "   Tool wear [min]  Type_H  Type_L  Type_M  Target  Failure Type_Heat  \\\n",
       "0               17       0       0       1       0                  0   \n",
       "1              118       0       0       1       0                  0   \n",
       "2               34       0       1       0       0                  0   \n",
       "3              149       0       0       1       0                  0   \n",
       "4               53       0       0       1       0                  0   \n",
       "\n",
       "   Failure Type_None  Failure Type_Overstrain  Failure Type_Power  \\\n",
       "0                  1                        0                   0   \n",
       "1                  1                        0                   0   \n",
       "2                  1                        0                   0   \n",
       "3                  1                        0                   0   \n",
       "4                  1                        0                   0   \n",
       "\n",
       "   Failure Type_Random  Failure Type_Tool  \n",
       "0                    0                  0  \n",
       "1                    0                  0  \n",
       "2                    0                  0  \n",
       "3                    0                  0  \n",
       "4                    0                  0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(cf.file_directory('cleaned') + 'train_df.csv')\n",
    "test_df = pd.read_csv(cf.file_directory('cleaned') + 'test_df.csv')\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7367114-e0f9-404d-a182-d70b77a2a8f7",
   "metadata": {},
   "source": [
    "## Train/test prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50403741-9541-4d07-bfc5-bc6f0e2f2f50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Process temperature [K]</th>\n",
       "      <th>Rotational speed [rpm]</th>\n",
       "      <th>Torque [Nm]</th>\n",
       "      <th>Tool wear [min]</th>\n",
       "      <th>Type_H</th>\n",
       "      <th>Type_L</th>\n",
       "      <th>Type_M</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>310.7</td>\n",
       "      <td>1454</td>\n",
       "      <td>39.4</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>309.7</td>\n",
       "      <td>1868</td>\n",
       "      <td>23.8</td>\n",
       "      <td>118</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>308.5</td>\n",
       "      <td>1616</td>\n",
       "      <td>30.2</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>312.6</td>\n",
       "      <td>1768</td>\n",
       "      <td>23.9</td>\n",
       "      <td>149</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>313.4</td>\n",
       "      <td>1624</td>\n",
       "      <td>32.1</td>\n",
       "      <td>53</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Process temperature [K]  Rotational speed [rpm]  Torque [Nm]  \\\n",
       "0                    310.7                    1454         39.4   \n",
       "1                    309.7                    1868         23.8   \n",
       "2                    308.5                    1616         30.2   \n",
       "3                    312.6                    1768         23.9   \n",
       "4                    313.4                    1624         32.1   \n",
       "\n",
       "   Tool wear [min]  Type_H  Type_L  Type_M  \n",
       "0               17       0       0       1  \n",
       "1              118       0       0       1  \n",
       "2               34       0       1       0  \n",
       "3              149       0       0       1  \n",
       "4               53       0       0       1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predictors\n",
    "cols = ['Process temperature [K]','Rotational speed [rpm]','Torque [Nm]','Tool wear [min]', 'Type_H', 'Type_L', 'Type_M']\n",
    "X_train = train_df[cols]\n",
    "X_test = test_df[cols]\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "092ed788-2481-462f-99a1-de19b760ac8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Mapping: {'Failure Type_Heat': 0, 'Failure Type_None': 1, 'Failure Type_Overstrain': 2, 'Failure Type_Power': 3, 'Failure Type_Random': 4, 'Failure Type_Tool': 5}\n"
     ]
    }
   ],
   "source": [
    "# Target\n",
    "y_train = train_df.filter(regex=(\"Failure Type.*\"))\n",
    "y_test = test_df.filter(regex=(\"Failure Type.*\"))\n",
    "\n",
    "# Reverse one hot encoding\n",
    "y_train = y_train.idxmax(axis=1)\n",
    "y_test = y_test.idxmax(axis=1)\n",
    "\n",
    "# Initialize LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Fit and transform the reversed target variable\n",
    "y_train = pd.Series(label_encoder.fit_transform(y_train))\n",
    "y_test = pd.Series(label_encoder.fit_transform(y_test))\n",
    "\n",
    "# Label mapping\n",
    "label_mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\n",
    "print(\"Label Mapping:\", label_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0adb74d2-e839-415b-9371-eafa93944cb3",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968e17d0-0ba4-4456-b41c-0a74d419925e",
   "metadata": {},
   "source": [
    "### Random forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ad340a-6604-4efd-a525-db425296cb81",
   "metadata": {},
   "source": [
    "**Hyperparameter selection**\n",
    "\n",
    "* The following hyperparameters are optimised:\n",
    "       \n",
    "    1. The number of estimators.\n",
    "       \n",
    "    2. The maximum depth of the trees.\n",
    "       \n",
    "        Note: If None, then nodes are expanded until all leaves are pure or until all leaves contain less than *min_samples_split* samples.\n",
    "\n",
    "    3. The minimum number of samples required to split an internal node is optimised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32c3bfa7-28b4-435b-88e9-70434d246085",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best n_estimators: 100\n",
      "Best max_depth: None\n",
      "Best min_samples_split: 5\n"
     ]
    }
   ],
   "source": [
    "# Create a pipeline with a scaler and classifier\n",
    "num_cols = ['Process temperature [K]','Rotational speed [rpm]','Torque [Nm]','Tool wear [min]']\n",
    "preprocessor = ColumnTransformer(transformers=[('scaler', MinMaxScaler(), num_cols)], remainder='passthrough')\n",
    "pipe = Pipeline([('preprocessor', preprocessor), ('rf', RandomForestClassifier())])\n",
    "\n",
    "# Define hyperparameters to tune\n",
    "param_grid = {\n",
    "    'rf__n_estimators': [100],\n",
    "    'rf__max_depth': [None, 10, 20],\n",
    "    'rf__min_samples_split': [2, 5, 10],\n",
    "}\n",
    "\n",
    "# Use StratifiedKFold for cross-validation to maintain class balance\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Create a custom scoring function for macro F1 score with subset of classes\n",
    "# Note: zero_division=0 means that if there are no predicted samples for a class, the precision, recall, or F1 score for that class will be set to 0\n",
    "subset_classes = [0, 2, 3, 4, 5] \n",
    "macro_f1_subset = make_scorer(f1_score, average='macro', labels=subset_classes,zero_division=0)\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "grid_search = GridSearchCV(pipe, param_grid, cv=cv, scoring=macro_f1_subset)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best estimator from grid search\n",
    "best_pipeline = grid_search.best_estimator_\n",
    "print(\"Best n_estimators:\", grid_search.best_params_['rf__n_estimators'])\n",
    "print(\"Best max_depth:\", grid_search.best_params_['rf__max_depth'])\n",
    "print(\"Best min_samples_split:\", grid_search.best_params_['rf__min_samples_split'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aaba0aa-1403-495f-aa49-ab0a29adf2e0",
   "metadata": {},
   "source": [
    "**Performance**\n",
    "\n",
    "* K-fold cross validation is used to assess the performance of the optimal configuration.\n",
    "\n",
    "* A stratified split is used to maintain the class balance.\n",
    "\n",
    "**_Key observations_**\n",
    "\n",
    "* Recall is $TBC$. This means the model identifies $TBC\\%$ of all failed machinery.\n",
    "\n",
    "    $Recall = \\frac{TP}{TP+FN}$\n",
    "\n",
    "* Precision is $TBC$. This means roughly $TBC\\%$ of failed predictions are correct.\n",
    "\n",
    "    $Precision = \\frac{TP}{TP+FP}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "598ca341-e8ea-4236-babf-c6e21b0f0fc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-Precision: 0.32620357420357415\n",
      "Macro-Recall: 0.24556288156288159\n",
      "Macro-F1: 0.523242087838059\n"
     ]
    }
   ],
   "source": [
    "best_params = {key.lstrip('rf__'):value for key, value in grid_search.best_params_.items()}\n",
    "\n",
    "# Use StratifiedKFold for cross-validation to maintain class balance\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Create custom macro scoring functions\n",
    "# Note: zero_division=0 means that if there are no predicted samples for a class, the precision, recall, or F1 score for that class will be set to 0\n",
    "macro_f1_subset = make_scorer(f1_score, average='macro', labels=subset_classes, zero_division=0)\n",
    "macro_precision_subset = make_scorer(precision_score, average='macro', labels=subset_classes, zero_division=0)\n",
    "macro_recall_subset = make_scorer(recall_score, average='macro', labels=subset_classes, zero_division=0)\n",
    "\n",
    "# Iterate over k-folds\n",
    "macro_precision, macro_recall, macro_f1 = [], [], []\n",
    "for train_index, test_index in cv.split(X_train, y_train):\n",
    "    X_train_k, y_train_k = X_train.loc[train_index], y_train.loc[train_index]\n",
    "    X_test_k, y_test_k = X_train.loc[test_index], y_train.loc[test_index]\n",
    "\n",
    "    # Create a pipeline with a classifier and a scaler\n",
    "    num_cols = ['Process temperature [K]','Rotational speed [rpm]','Torque [Nm]','Tool wear [min]']\n",
    "    preprocessor = ColumnTransformer(transformers=[('scaler', MinMaxScaler(), num_cols)], remainder='passthrough')\n",
    "    pipe = Pipeline([('preprocessor', preprocessor), ('rf', RandomForestClassifier(**best_params))])\n",
    "\n",
    "    # Perform predictions\n",
    "    pipe.fit(X_train_k, y_train_k)\n",
    "    \n",
    "    # Calculate precision, recall & f1-score\n",
    "    precision_k = macro_precision_subset(pipe, X_test_k, y_test_k)\n",
    "    recall_k = macro_recall_subset(pipe, X_test_k, y_test_k)\n",
    "    f1_k = macro_f1_subset(pipe, X_test_k, y_test_k)\n",
    "\n",
    "    macro_precision.append(precision_k)\n",
    "    macro_recall.append(recall_k)\n",
    "    macro_f1.append(f1_k)\n",
    "\n",
    "print(f\"Macro-Precision: {np.mean(macro_precision)}\")\n",
    "print(f\"Macro-Recall: {np.mean(macro_recall)}\")\n",
    "print(f\"Macro-F1: {np.sqrt(np.mean(macro_f1))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab906a2-5713-4e6b-bfbf-6d6fb0e2421d",
   "metadata": {},
   "source": [
    "### Random Forest with SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9904afc-b728-49a2-b695-736a0dc4126c",
   "metadata": {},
   "source": [
    "**Hyperparameter selection**\n",
    "\n",
    "* The following hyperparameters are optimised:\n",
    "       \n",
    "    1. The number of estimators.\n",
    "       \n",
    "    2. The maximum depth of the trees.\n",
    "       \n",
    "        Note: If None, then nodes are expanded until all leaves are pure or until all leaves contain less than *min_samples_split* samples.\n",
    "\n",
    "    3. The minimum number of samples required to split an internal node is optimised.\n",
    " \n",
    "    4. The SMOTE sampling strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b31f2c37-4ef4-4f85-ac6f-63a304f11210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset shape: Counter({1: 6456, 0: 80, 3: 69, 2: 49, 5: 33, 4: 13})\n",
      "Resampled: Counter({1: 6456, 3: 6456, 0: 6456, 2: 6456, 4: 6456, 5: 6456})\n"
     ]
    }
   ],
   "source": [
    "# Test smote resampling\n",
    "print(f\"Original dataset shape: {collections.Counter(y_train)}\")\n",
    "sm = SMOTE(random_state=42)\n",
    "X_res, y_res = sm.fit_resample(X_train, y_train)\n",
    "print(f\"Resampled: {collections.Counter(y_res)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797283cc-304e-4f0b-972f-266496191614",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline with a scaler, SMOTE and classifier\n",
    "num_cols = ['Process temperature [K]','Rotational speed [rpm]','Torque [Nm]','Tool wear [min]']\n",
    "preprocessor = ColumnTransformer(transformers=[('scaler', MinMaxScaler(), num_cols)], remainder='passthrough')\n",
    "pipe = make_pipeline(preprocessor, SMOTE(random_state=42), RandomForestClassifier())\n",
    "\n",
    "# Define hyperparameters to tune\n",
    "param_grid = {\n",
    "    'randomforestclassifier__n_estimators': [50, 100, 150],\n",
    "    'randomforestclassifier__max_depth': [None, 10, 20],\n",
    "    'randomforestclassifier__min_samples_split': [20, 30, 40],\n",
    "}\n",
    "\n",
    "# Use StratifiedKFold for cross-validation to maintain class balance\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Create a custom scoring function for macro F1 score with subset of classes\n",
    "# Note: zero_division=0 means that if there are no predicted samples for a class, the precision, recall, or F1 score for that class will be set to 0\n",
    "subset_classes = [0, 2, 3, 4, 5] \n",
    "macro_f1_subset = make_scorer(f1_score, average='macro', labels=subset_classes, zero_division=0)\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "grid_search = GridSearchCV(pipe, param_grid, cv=cv, scoring=macro_f1_subset)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best estimator from grid search\n",
    "best_pipeline = grid_search.best_estimator_\n",
    "\n",
    "# Print the best parameters\n",
    "print(\"Best n_estimators:\", grid_search.best_params_['randomforestclassifier__n_estimators'])\n",
    "print(\"Best max_depth:\", grid_search.best_params_['randomforestclassifier__max_depth'])\n",
    "print(\"Best min_samples_split:\", grid_search.best_params_['randomforestclassifier__min_samples_split'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d567ca-0fbb-4200-96b5-06d7a99df550",
   "metadata": {},
   "source": [
    "**Performance**\n",
    "\n",
    "* K-fold cross validation is used to assess the performance of the optimal configuration.\n",
    "\n",
    "* A stratified split is used to maintain the class balance.\n",
    "\n",
    "**_Key observations_**\n",
    "\n",
    "* Recall is $TBC$. This means the model identifies $TBC\\%$ of all failed machinery.\n",
    "\n",
    "    $Recall = \\frac{TP}{TP+FN}$\n",
    "\n",
    "* Precision is $TBC$. This means roughly $TBC\\%$ of failed predictions are correct.\n",
    "\n",
    "    $Precision = \\frac{TP}{TP+FP}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c22cbe-abab-4a5c-ae07-9da5e6d4e0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_best_params = {key:value for key, value in grid_search.best_params_.items() if 'randomforestclassifier__' in key}\n",
    "rf_best_params = {key.replace('randomforestclassifier__',''):value for key, value in rf_best_params.items()}\n",
    "\n",
    "# Use StratifiedKFold for cross-validation to maintain class balance\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Create custom macro scoring functions\n",
    "# Note: zero_division=0 means that if there are no predicted samples for a class, the precision, recall, or F1 score for that class will be set to 0\n",
    "macro_f1_subset = make_scorer(f1_score, average='macro', labels=subset_classes, zero_division=0)\n",
    "macro_precision_subset = make_scorer(precision_score, average='macro', labels=subset_classes, zero_division=0)\n",
    "macro_recall_subset = make_scorer(recall_score, average='macro', labels=subset_classes, zero_division=0)\n",
    "\n",
    "# Iterate over k-folds\n",
    "macro_precision, macro_recall, macro_f1 = [], [], []\n",
    "for train_index, test_index in cv.split(X_train, y_train):\n",
    "    X_train_k, y_train_k = X_train_scaled.loc[train_index], y_train_scaled.loc[train_index]\n",
    "    X_test_k, y_test_k = X_train_scaled.loc[test_index], y_train_scaled.loc[test_index]\n",
    "\n",
    "    # Convert y_train_k and y_test_k to NumPy arrays\n",
    "    y_train_k_np = np.array(y_train_k)\n",
    "    y_test_k_np = np.array(y_test_k)\n",
    "\n",
    "    # Create a pipeline with a classifier and a scaler\n",
    "    num_cols = ['Process temperature [K]','Rotational speed [rpm]','Torque [Nm]','Tool wear [min]']\n",
    "    preprocessor = ColumnTransformer(transformers=[('scaler', MinMaxScaler(), num_cols)], remainder='passthrough')\n",
    "    pipe = make_pipeline(preprocessor, SMOTE(random_state=42), RandomForestClassifier(**rf_best_params))\n",
    "\n",
    "    # Perform predictions\n",
    "    pipe.fit(X_train_k, y_train_k_np)\n",
    "    \n",
    "    # Calculate precision, recall & f1-score\n",
    "    precision_k = macro_precision_subset(pipe, X_test_k, y_test_k_np)\n",
    "    recall_k = macro_recall_subset(pipe, X_test_k, y_test_k_np)\n",
    "    f1_k = macro_f1_subset(pipe, X_test_k, y_test_k_np)\n",
    "\n",
    "    macro_precision.append(precision_k)\n",
    "    macro_recall.append(recall_k)\n",
    "    macro_f1.append(f1_k)\n",
    "\n",
    "print(f\"Macro-Precision: {np.mean(macro_precision)}\")\n",
    "print(f\"Macro-Recall: {np.mean(macro_recall)}\")\n",
    "print(f\"Macro-F1: {np.sqrt(np.mean(macro_f1))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1dbed4-3485-4c28-b2f9-01dc7b785b5e",
   "metadata": {},
   "source": [
    "## Model evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d90f630-5cbc-47f8-92c5-4865e775ab8d",
   "metadata": {},
   "source": [
    "| Model | Recall | Precision | AUC |\n",
    "|----------|----------|----------|----------|\n",
    "| Random Forest   | TBC   | TBC   | TBC   |\n",
    "| Random Forest with SMOTE   | TBC   | TBC   | TBC   |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7351f2-c7ba-4fae-b9da-7bc3af6fb893",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrain with entire training set\n",
    "num_cols = ['Process temperature [K]','Rotational speed [rpm]','Torque [Nm]','Tool wear [min]']\n",
    "preprocessor = ColumnTransformer(transformers=[('scaler', MinMaxScaler(), num_cols)], remainder='passthrough')\n",
    "pipe = make_pipeline(preprocessor, SMOTE(random_state=42), RandomForestClassifier(**rf_best_params))\n",
    "pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dbd9672-e94e-4bc4-954f-e12c4b9374e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate with test set\n",
    "\n",
    "# Predict\n",
    "y_pred = pipe.predict(X_test)\n",
    "\n",
    "# Classification report\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Confusion matrix\n",
    "# Source: https://medium.com/@dtuk81/confusion-matrix-visualization-fc31e3f30fea\n",
    "confusion_matrices_count = confusion_matrix(y_test, y_pred)\n",
    "confusion_matrices_perc = confusion_matrix(y_test, y_pred, normalize='true')*100\n",
    "\n",
    "# Plot confusion matrix\n",
    "confusion_matrices_perc  = confusion_matrices_perc .round(1)\n",
    "labels = [f\"{c}\\n{p}%\" for c, p in zip(confusion_matrices_count.flatten(), confusion_matrices_perc.flatten())]\n",
    "labels = np.asarray(labels).reshape(6, 6)\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=1, constrained_layout=True, figsize=(6.4, 4.8))\n",
    "axes.tick_params(bottom=False, left=False)\n",
    "axes.grid(False)\n",
    "axes.spines[['top', 'right']].set_visible(False)\n",
    "axes.spines[['left', 'bottom']].set_color('dimgray')\n",
    "\n",
    "sns.heatmap(confusion_matrices_perc, ax=axes, annot=labels, fmt=\"s\", annot_kws={\"fontsize\":10})\n",
    "axes.set_xlabel('Actual',fontsize=15, color='dimgray')\n",
    "axes.set_ylabel('Predicted',fontsize=15, color='dimgray');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa09410-965c-46f7-9817-5439506302c7",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "topic_analyses",
   "language": "python",
   "name": "topic_analyses"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
